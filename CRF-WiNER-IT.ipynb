{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CRF for Entity Extraction on WikiNER (Italian)\n",
    "WiNER is a dataset of annotated sentences for Entity Extraction taken from Wikipedia. In this notebook we train and evaluate a CRF model on the italian data to recognize entities such as Persons, Locations and Orgnizations from text.\n",
    "\n",
    "We use the `sklearn-crfsuite` package for implementing our model and `seqeval` for f1-score evaluation.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from utils import dataio, modelutils\n",
    "from pprint import pprint\n",
    "from sklearn_crfsuite import CRF\n",
    "from sklearn.model_selection import train_test_split\n",
    "from seqeval.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the dataset from the `data/` directory. For each token, the datatset reports word, Part of Speech tag and entity tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 127940 sentences.\n"
     ]
    }
   ],
   "source": [
    "file_path = os.path.join('data', 'wikiner-it-wp3-raw.txt')\n",
    "sentences, tags, output_labels = dataio.load_wikiner(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: {'I-LOC', 'I-ORG', 'O', 'B-PER', 'I-MISC', 'B-LOC', 'B-MISC', 'I-PER', 'B-ORG'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Labels:\", output_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Example:\n",
      "[('Seguirono', 'VER:remo'),\n",
      " ('Lamarck', 'NOM'),\n",
      " ('(', 'PON'),\n",
      " ('1744', 'NUM'),\n",
      " ('--', 'NOM'),\n",
      " ('1829', 'NUM'),\n",
      " (')', 'PON'),\n",
      " (',', 'PON'),\n",
      " ('Blumenbach', 'NOM'),\n",
      " ('(', 'PON'),\n",
      " ('1752', 'NUM'),\n",
      " ('--', 'NOM'),\n",
      " ('1840', 'NUM'),\n",
      " (')', 'PON'),\n",
      " (',', 'PON'),\n",
      " ('con', 'PRE'),\n",
      " ('le', 'DET:def'),\n",
      " ('sue', 'PRO:poss'),\n",
      " ('norme', 'NOM'),\n",
      " ('descrittive', 'ADJ'),\n",
      " ('del', 'PRE:det'),\n",
      " ('cranio', 'NOM'),\n",
      " (',', 'PON'),\n",
      " ('Paul', 'NPR'),\n",
      " ('Broca', 'NOM'),\n",
      " ('con', 'PRE'),\n",
      " ('la', 'DET:def'),\n",
      " ('focalizzazione', 'NOM'),\n",
      " ('dei', 'PRE:det'),\n",
      " ('rapporti', 'NOM'),\n",
      " ('tra', 'PRE'),\n",
      " ('morfologia', 'NOM'),\n",
      " ('e', 'CON'),\n",
      " ('funzionalitÃ ', 'NOM'),\n",
      " ('.', 'SENT')]\n",
      "==============================\n",
      "['O', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "print(\"Sentence Example:\")\n",
    "pprint(sentences[1])\n",
    "print(\"=\"*30)\n",
    "print(tags[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features Engineering\n",
    "\n",
    "In this section, we build our feature vector for each token. It is composed by:\n",
    "* The lowercase token string*;\n",
    "* The token suffix;\n",
    "* If the token is capitalized*;\n",
    "* If the token is uppercase*;\n",
    "* If the token is a number;\n",
    "* Complete Part-of-Speech tag of the token*;\n",
    "* More general Part-of-Speech tag of the token*;\n",
    "* If the token is the first of the sentence;\n",
    "* If the token is the last of the sentence.\n",
    "\n",
    "\\* also for previous and next tokens, if there are.  \n",
    "\n",
    "> Note: categorical features are one-hot encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_features(sentence, idx):\n",
    "    \"\"\"Extract features related to a word and its neighbours\"\"\"\n",
    "    word, pos = sentence[idx]\n",
    "    \n",
    "    features = {\n",
    "        'bias': 1.0,\n",
    "        'word.lower()': word.lower(),\n",
    "        'word[-3:]': word[-3:],\n",
    "        'word[-2:]': word[-2:],\n",
    "        'word.isupper()': word.isupper(),\n",
    "        'word.istitle()': word.istitle(),\n",
    "        'word.isdigit()': word.isdigit(),\n",
    "        'postag': pos,\n",
    "        'postag[:2]': pos[:2],\n",
    "    }\n",
    "    if idx > 0:\n",
    "        word1, pos1 = sentence[idx-1]\n",
    "        features.update({\n",
    "            '-1:word.lower()': word1.lower(),\n",
    "            '-1:word.istitle()': word1.istitle(),\n",
    "            '-1:word.isupper()': word1.isupper(),\n",
    "            '-1:postag': pos1,\n",
    "            '-1:postag[:2]': pos1[:2],\n",
    "        })\n",
    "    else:\n",
    "        features['BOS'] = True\n",
    "        \n",
    "    if idx < len(sentence)-1:\n",
    "        word1, pos1 = sentence[idx+1]\n",
    "        features.update({\n",
    "            '+1:word.lower()': word1.lower(),\n",
    "            '+1:word.istitle()': word1.istitle(),\n",
    "            '+1:word.isupper()': word1.isupper(),\n",
    "            '+1:postag': pos1,\n",
    "            '+1:postag[:2]': pos1[:2],\n",
    "        })\n",
    "    else:\n",
    "        features['EOS'] = True\n",
    "                \n",
    "    return features\n",
    "\n",
    "\n",
    "def sentence_features(sentence):\n",
    "    return tuple(word_features(sentence, index) for index in range(len(sentence)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [sentence_features(sentence) for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token features example:\n",
      "{'+1:postag': 'PON',\n",
      " '+1:postag[:2]': 'PO',\n",
      " '+1:word.istitle()': False,\n",
      " '+1:word.isupper()': False,\n",
      " '+1:word.lower()': '(',\n",
      " '-1:postag': 'VER:remo',\n",
      " '-1:postag[:2]': 'VE',\n",
      " '-1:word.istitle()': True,\n",
      " '-1:word.isupper()': False,\n",
      " '-1:word.lower()': 'seguirono',\n",
      " 'bias': 1.0,\n",
      " 'postag': 'NOM',\n",
      " 'postag[:2]': 'NO',\n",
      " 'word.isdigit()': False,\n",
      " 'word.istitle()': True,\n",
      " 'word.isupper()': False,\n",
      " 'word.lower()': 'lamarck',\n",
      " 'word[-2:]': 'ck',\n",
      " 'word[-3:]': 'rck'}\n",
      "==============================\n",
      "I-PER\n"
     ]
    }
   ],
   "source": [
    "print(\"Token features example:\")\n",
    "pprint(X[1][1])\n",
    "print(\"=\"*30)\n",
    "print(tags[1][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, tags, test_size=0.2, \n",
    "                                                    random_state=3791)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, \n",
    "                                                      test_size=0.2, \n",
    "                                                      random_state=3791)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 32min 14s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\srichetti\\AppData\\Local\\Continuum\\anaconda3\\envs\\ner-suite\\lib\\site-packages\\sklearn\\base.py:213: FutureWarning: From version 0.24, get_params will raise an AttributeError if a parameter cannot be retrieved as an instance attribute. Previously it would return None.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CRF(algorithm='lbfgs', all_possible_transitions=True, c1=0.1, c2=0.5,\n",
       "    keep_tempfiles=None, max_iterations=800)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "crf = CRF(\n",
    "    algorithm = 'lbfgs',\n",
    "    c1 = 0.1,\n",
    "    c2 = 0.5,\n",
    "    max_iterations = 800,\n",
    "    all_possible_transitions = True,\n",
    "    verbose = False\n",
    ")\n",
    "\n",
    "crf.fit(X_train, y_train, X_dev=X_valid, y_dev=y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "We evaluate:\n",
    "* **Memory consumption** using the attribute `crf.size_`;\n",
    "* **Latency in prediction** using the function `time.process_time()`;\n",
    "* **F1-score** _on entities_ on the test set using `seqeval`;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 7.54M\n"
     ]
    }
   ],
   "source": [
    "print('Model size: {:0.2f}M'.format(crf.size_ / 1000000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model latency in prediction: 0.000309 s\n"
     ]
    }
   ],
   "source": [
    "print(f'Model latency in prediction: {modelutils.compute_prediction_latency(X_test, crf):.3} s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set\n",
      "           precision    recall  f1-score   support\n",
      "\n",
      "      LOC      0.898     0.933     0.915     82830\n",
      "      ORG      0.923     0.804     0.859     13708\n",
      "     MISC      0.893     0.762     0.822     24386\n",
      "      PER      0.940     0.931     0.935     46049\n",
      "\n",
      "micro avg      0.911     0.897     0.904    166973\n",
      "macro avg      0.911     0.897     0.902    166973\n",
      "\n",
      "\n",
      "\n",
      "Test Set\n",
      "           precision    recall  f1-score   support\n",
      "\n",
      "      LOC      0.865     0.899     0.882     25889\n",
      "      ORG      0.861     0.729     0.789      4224\n",
      "      PER      0.903     0.896     0.900     14193\n",
      "     MISC      0.795     0.656     0.719      7402\n",
      "\n",
      "micro avg      0.867     0.850     0.858     51708\n",
      "macro avg      0.865     0.850     0.856     51708\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datasets = [('Training Set', X_train, y_train), ('Test Set', X_test, y_test)]\n",
    "\n",
    "for title, X, Y in datasets:\n",
    "    Y_pred = crf.predict(X)\n",
    "    print(title)\n",
    "    print(classification_report(Y, Y_pred, digits=3))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
