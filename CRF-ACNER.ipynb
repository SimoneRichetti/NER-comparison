{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CRF for Entity Extraction on Annotated Corpus for Named Entity Recognition\n",
    "\n",
    "In this notebook we build a CRF model for Named Entity Recognition over the [ACNER](https://www.kaggle.com/abhinavwalia95/entity-annotated-corpus) dataset from Kaggle. \n",
    "\n",
    "We use the `sklearn-crfsuite` package for implementing our model and `seqeval` for f1-score evaluation.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from utils import dataio, modelutils\n",
    "from pprint import pprint\n",
    "from sklearn_crfsuite import CRF\n",
    "from sklearn.model_selection import train_test_split\n",
    "from seqeval.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "Load dataset from the `data/` directory and extract only chosen features.\n",
    "For each token, the feature vector is composed by:\n",
    "* The string of the token*;\n",
    "* Token lemma*;\n",
    "* Token Part-of-Speech tag*;\n",
    "* Token shape (uppercase, lowercase, capitalized, punctuation, ...)*;\n",
    "* Sentence Index.\n",
    "\n",
    "\\* also for previous and next tokens. If the token is the first or the last of a sentence, value for previous/next token are replaced with a special value (`__start__` and `__end__`)\n",
    "\n",
    "> Note: categorical features are one-hot encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 281837: expected 25 fields, saw 34\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filter level: default\n",
      "Features: Index(['lemma', 'next-lemma', 'next-pos', 'next-shape', 'next-word', 'pos',\n",
      "       'prev-lemma', 'prev-pos', 'prev-shape', 'prev-word', 'sentence_idx',\n",
      "       'shape', 'word', 'tag'],\n",
      "      dtype='object')\n",
      "Dataset dimension: 35177 sentences\n",
      "Data read successfully!\n"
     ]
    }
   ],
   "source": [
    "X, y, tags = dataio.load_anerd_data(os.path.join('data', 'annotated-ner-dataset', 'ner.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels:\n",
      "{'B-art',\n",
      " 'B-eve',\n",
      " 'B-geo',\n",
      " 'B-gpe',\n",
      " 'B-nat',\n",
      " 'B-org',\n",
      " 'B-per',\n",
      " 'B-tim',\n",
      " 'I-art',\n",
      " 'I-eve',\n",
      " 'I-geo',\n",
      " 'I-gpe',\n",
      " 'I-nat',\n",
      " 'I-org',\n",
      " 'I-per',\n",
      " 'I-tim',\n",
      " 'O',\n",
      " 'unk'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Labels:\")\n",
    "pprint(tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test Split\n",
    "\n",
    "Split data into training set and test set. We set a fixed random state in order to easily reproduce results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token features example:\n",
      "{'lemma': 'offici',\n",
      " 'next-lemma': 'announc',\n",
      " 'next-pos': 'VBD',\n",
      " 'next-shape': 'lowercase',\n",
      " 'next-word': 'announced',\n",
      " 'pos': 'NNS',\n",
      " 'prev-lemma': 'turkish',\n",
      " 'prev-pos': 'JJ',\n",
      " 'prev-shape': 'capitalized',\n",
      " 'prev-word': 'Turkish',\n",
      " 'sentence_idx': 6742.0,\n",
      " 'shape': 'lowercase',\n",
      " 'word': 'officials'}\n",
      "==============================\n",
      "O\n"
     ]
    }
   ],
   "source": [
    "print(\"Token features example:\")\n",
    "pprint(X_train[0][1])\n",
    "print(\"=\"*30)\n",
    "print(y_train[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "\n",
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 15min 46s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\srichetti\\AppData\\Local\\Continuum\\anaconda3\\envs\\ner-suite\\lib\\site-packages\\sklearn\\base.py:213: FutureWarning: From version 0.24, get_params will raise an AttributeError if a parameter cannot be retrieved as an instance attribute. Previously it would return None.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CRF(algorithm='lbfgs', all_possible_transitions=True, c1=0.1, c2=0.5,\n",
       "    keep_tempfiles=None, max_iterations=800)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "crf = CRF(\n",
    "    algorithm = 'lbfgs',\n",
    "    c1 = 0.1,\n",
    "    c2 = 0.5,\n",
    "    max_iterations = 800,\n",
    "    all_possible_transitions = True,\n",
    "    verbose = False\n",
    ")\n",
    "\n",
    "crf.fit(X_train, y_train, X_dev=X_valid, y_dev=y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "We evaluate:\n",
    "* **Memory consumption** using the attribute `crf.size_`;\n",
    "* **Latency in prediction** using the function `time.process_time()`;\n",
    "* **F1-score** _on entities_ on the test set using `seqeval`;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 4.04M\n"
     ]
    }
   ],
   "source": [
    "print('Model size: {:0.2f}M'.format(crf.size_ / 1000000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model latency in prediction: 0.00026 s\n"
     ]
    }
   ],
   "source": [
    "print(f'Model latency in prediction: {modelutils.compute_prediction_latency(X_test, crf):.3} s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set\n",
      "           precision    recall  f1-score   support\n",
      "\n",
      "      org      0.905     0.843     0.873     12900\n",
      "      geo      0.902     0.952     0.926     23737\n",
      "      gpe      0.982     0.940     0.960     10450\n",
      "      tim      0.958     0.910     0.933     12878\n",
      "      per      0.911     0.886     0.898     10949\n",
      "      art      0.973     0.602     0.744       304\n",
      "      nat      0.924     0.660     0.770       147\n",
      "      eve      0.913     0.796     0.851       211\n",
      "\n",
      "micro avg      0.925     0.910     0.918     71576\n",
      "macro avg      0.926     0.910     0.917     71576\n",
      "\n",
      "\n",
      "\n",
      "Test Set\n",
      "           precision    recall  f1-score   support\n",
      "\n",
      "      geo      0.842     0.901     0.871      7715\n",
      "      gpe      0.966     0.925     0.945      3305\n",
      "      per      0.782     0.761     0.771      3289\n",
      "      org      0.762     0.698     0.729      3983\n",
      "      tim      0.901     0.843     0.871      4053\n",
      "      art      0.000     0.000     0.000        75\n",
      "      eve      0.610     0.357     0.450        70\n",
      "      nat      0.769     0.408     0.533        49\n",
      "\n",
      "micro avg      0.847     0.832     0.839     22539\n",
      "macro avg      0.844     0.832     0.837     22539\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datasets = [('Training Set', X_train, y_train), ('Test Set', X_test, y_test)]\n",
    "\n",
    "for title, X, Y in datasets:\n",
    "    Y_pred = crf.predict(X)\n",
    "    print(title)\n",
    "    print(classification_report(Y, Y_pred, digits=3))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
