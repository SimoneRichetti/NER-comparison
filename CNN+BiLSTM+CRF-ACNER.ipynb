{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s6Oi4OFtlMFM"
   },
   "source": [
    "# CNN + Glove + BiLSTM + CRF model for Entity Extraction on ACNER\n",
    "\n",
    "In this notebook, I implement the neural model described in [this paper](https://www.aclweb.org/anthology/P16-1101.pdf). This model uses:\n",
    "* Character-level informations extracted with a CNN;\n",
    "* Word-level informations starting from Glove 100-dimensional 6B embedding;\n",
    "* A BiLSTM and a CRF layer for making predictions.\n",
    "\n",
    "Data preprocessing is composed of padding sentences plus token encoding and character-sequences padding to fixed length. Then, we implement this model using `tensorflow.keras` and the `tf2crf` package for a CRF layer compatible with tensorflow. We test it on the Annotated Corpus for NER dataset, using the `seqeval` package for f1-score evaluation.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-nmzyeF3u_Ni"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "import numpy as np\n",
    "from utils import dataio, kerasutils, modelutils\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RzksTlOXmRrW"
   },
   "source": [
    "## Load Dataset\n",
    "The dataset can be found [here](https://www.kaggle.com/abhinavwalia95/entity-annotated-corpus). It reports a lot of features for each token, but we only keep the token string and the entity tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118
    },
    "colab_type": "code",
    "id": "lyW9pOkIyCE-",
    "outputId": "86507bba-e635-4cd1-fcb0-bffb1fe0a8e1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 281837: expected 25 fields, saw 34\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filter level: sentence_only\n",
      "Dataset dimension: 35177 sentences\n",
      "Data read successfully!\n"
     ]
    }
   ],
   "source": [
    "raw, ner, output_labels = dataio.load_anerd_data(os.path.join('data', 'annotated-ner-dataset', 'ner.csv'),\n",
    "                                           filter_level='sentence_only')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: {'I-nat', 'B-gpe', 'B-org', 'I-org', 'I-eve', 'I-gpe', 'B-art', 'I-geo', 'B-tim', 'B-per', 'B-geo', 'I-art', 'I-per', 'I-tim', 'O', 'unk', 'B-nat', 'B-eve'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Labels:\", output_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Example:\n",
      "Thousands       | O\n",
      "of              | O\n",
      "demonstrators   | O\n",
      "have            | O\n",
      "marched         | O\n",
      "through         | O\n",
      "London          | B-geo\n",
      "to              | O\n",
      "protest         | O\n",
      "the             | O\n",
      "war             | O\n",
      "in              | O\n",
      "Iraq            | B-geo\n",
      "and             | O\n",
      "demand          | O\n",
      "the             | O\n",
      "withdrawal      | O\n",
      "of              | O\n",
      "British         | B-gpe\n",
      "troops          | O\n",
      "from            | O\n",
      "that            | O\n",
      "country         | O\n",
      ".               | O\n",
      "Thousands       | O\n",
      "of              | O\n",
      "demonstrators   | O\n",
      "have            | O\n",
      "marched         | O\n",
      "through         | O\n",
      "London          | B-geo\n",
      "to              | O\n",
      "protest         | O\n",
      "the             | O\n",
      "war             | O\n",
      "in              | O\n",
      "Iraq            | B-geo\n",
      "and             | O\n",
      "demand          | O\n",
      "the             | O\n",
      "withdrawal      | O\n",
      "of              | O\n",
      "British         | B-gpe\n",
      "troops          | O\n",
      "from            | O\n",
      "that            | O\n",
      "country         | O\n",
      ".               | O\n"
     ]
    }
   ],
   "source": [
    "print(\"Sentence Example:\")\n",
    "for i in range(len(raw[0])):\n",
    "    print(f'{raw[0][i]:15} | {ner[0][i]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "opnm9IMny5ID"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kjmchvML7JWw"
   },
   "source": [
    "# Data Preparation\n",
    "Prepare character- and word-level input for the model.\n",
    "\n",
    "## Sentence encoding and padding\n",
    "We use a Keras `Tokenizer` to extract the vocabulary and encode words. We pad sentences to a fixed length because it is required from LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mgkojyV47DDQ"
   },
   "outputs": [],
   "source": [
    "# integer encode sequences of words\n",
    "token_tokenizer = Tokenizer()    # Automatically lowers tokens\n",
    "token_tokenizer.fit_on_texts(raw)\n",
    "sequences = token_tokenizer.texts_to_sequences(raw)\n",
    "\n",
    "# Label encoding\n",
    "tag2idx = { tag: idx for idx, tag in enumerate(output_labels) }\n",
    "idx2tag = { idx: tag for tag, idx in tag2idx.items() }\n",
    "ner_sequences = [[tag2idx[tag] for tag in sentence] for sentence in ner]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "HVOgrGyg5D06",
    "outputId": "b764bd2b-3e1c-4fa6-c455-c5087e8b35c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27419\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(token_tokenizer.word_counts)\n",
    "print(vocabulary_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XhUgVKH85GU_"
   },
   "outputs": [],
   "source": [
    "max_sentence_len = 60\n",
    "X_sent = pad_sequences(sequences, maxlen=max_sentence_len, padding='post', truncating='post')\n",
    "Y = pad_sequences(ner_sequences, maxlen=max_sentence_len, value=tag2idx['O'], padding='post', truncating='post')\n",
    "\n",
    "X_sent = np.array(X_sent)\n",
    "Y = np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "TYX5wdr75Zp-",
    "outputId": "5a204f82-2574-4aa6-c07a-14548b4f93ab"
   },
   "outputs": [],
   "source": [
    "token_tokenizer.index_word[0] = '_PAD_'\n",
    "token_tokenizer.word_index['_PAD_'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "TYX5wdr75Zp-",
    "outputId": "5a204f82-2574-4aa6-c07a-14548b4f93ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded and padded sentence:\n",
      "   259 | thousands\n",
      "     5 | of\n",
      "   902 | demonstrators\n",
      "    15 | have\n",
      "  1950 | marched\n",
      "   245 | through\n",
      "   482 | london\n",
      "     6 | to\n",
      "   492 | protest\n",
      "     1 | the\n",
      "   134 | war\n",
      "     4 | in\n",
      "    59 | iraq\n",
      "     8 | and\n",
      "   640 | demand\n",
      "     1 | the\n",
      "   799 | withdrawal\n",
      "     5 | of\n",
      "   182 | british\n",
      "    91 | troops\n"
     ]
    }
   ],
   "source": [
    "print('Encoded and padded sentence:')\n",
    "for i in range(len(X_sent[0][:20])):\n",
    "    print(f'{X_sent[0][i]:6} | {token_tokenizer.index_word[X_sent[0][i]]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t3FaEIl15nvp"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RwKih7le7Oic"
   },
   "source": [
    "## Character encoding and padding\n",
    "In order to extract character-level informations, we have to:\n",
    "* Encode characters with integers;\n",
    "* Pad words to a fixed lengths;\n",
    "* Use the 0 as padding integer both for sentence padding and for word padding.\n",
    "\n",
    "We don't want to truncate words because prefix and suffix contains precious informations, so we take the longest words and we pad words to its length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yENsgixv-kmm"
   },
   "outputs": [],
   "source": [
    "def to_char_list(data):\n",
    "    '''Transform all the words of a dataset into lists of characters'''\n",
    "    \n",
    "    char_data = []\n",
    "    for sentence in data:\n",
    "        char_sent = []\n",
    "        for word in sentence:\n",
    "            char_sent.append(list(word))\n",
    "        char_data.append(char_sent)\n",
    "    return char_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "ISB3Hepk5Yro",
    "outputId": "e3d2cc7b-7c9d-4de1-e263-91c51345ebc2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['T', 'h', 'o', 'u', 's', 'a', 'n', 'd', 's']\n",
      "['o', 'f']\n",
      "['d', 'e', 'm', 'o', 'n', 's', 't', 'r', 'a', 't', 'o', 'r', 's']\n",
      "['h', 'a', 'v', 'e']\n",
      "['m', 'a', 'r', 'c', 'h', 'e', 'd']\n",
      "['t', 'h', 'r', 'o', 'u', 'g', 'h']\n",
      "['L', 'o', 'n', 'd', 'o', 'n']\n",
      "['t', 'o']\n",
      "['p', 'r', 'o', 't', 'e', 's', 't']\n",
      "['t', 'h', 'e']\n",
      "['w', 'a', 'r']\n",
      "['i', 'n']\n",
      "['I', 'r', 'a', 'q']\n",
      "['a', 'n', 'd']\n",
      "['d', 'e', 'm', 'a', 'n', 'd']\n",
      "['t', 'h', 'e']\n",
      "['w', 'i', 't', 'h', 'd', 'r', 'a', 'w', 'a', 'l']\n",
      "['o', 'f']\n",
      "['B', 'r', 'i', 't', 'i', 's', 'h']\n",
      "['t', 'r', 'o', 'o', 'p', 's']\n",
      "['f', 'r', 'o', 'm']\n",
      "['t', 'h', 'a', 't']\n",
      "['c', 'o', 'u', 'n', 't', 'r', 'y']\n",
      "['.']\n",
      "['T', 'h', 'o', 'u', 's', 'a', 'n', 'd', 's']\n",
      "['o', 'f']\n",
      "['d', 'e', 'm', 'o', 'n', 's', 't', 'r', 'a', 't', 'o', 'r', 's']\n",
      "['h', 'a', 'v', 'e']\n",
      "['m', 'a', 'r', 'c', 'h', 'e', 'd']\n",
      "['t', 'h', 'r', 'o', 'u', 'g', 'h']\n",
      "['L', 'o', 'n', 'd', 'o', 'n']\n",
      "['t', 'o']\n",
      "['p', 'r', 'o', 't', 'e', 's', 't']\n",
      "['t', 'h', 'e']\n",
      "['w', 'a', 'r']\n",
      "['i', 'n']\n",
      "['I', 'r', 'a', 'q']\n",
      "['a', 'n', 'd']\n",
      "['d', 'e', 'm', 'a', 'n', 'd']\n",
      "['t', 'h', 'e']\n",
      "['w', 'i', 't', 'h', 'd', 'r', 'a', 'w', 'a', 'l']\n",
      "['o', 'f']\n",
      "['B', 'r', 'i', 't', 'i', 's', 'h']\n",
      "['t', 'r', 'o', 'o', 'p', 's']\n",
      "['f', 'r', 'o', 'm']\n",
      "['t', 'h', 'a', 't']\n",
      "['c', 'o', 'u', 'n', 't', 'r', 'y']\n",
      "['.']\n",
      "==============================\n",
      "35177\n"
     ]
    }
   ],
   "source": [
    "raw_char = to_char_list(raw)\n",
    "\n",
    "for token in raw_char[0]:\n",
    "    print(token)\n",
    "print('='*30)\n",
    "print(len(raw_char))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LRBCQI7DwudX"
   },
   "outputs": [],
   "source": [
    "# Sanity check of preprocessed data dimensions. If it does not output anything,\n",
    "# everything is fine.\n",
    "for sent_idx in range(len(raw)):\n",
    "    if len(raw_char[sent_idx]) != len(sequences[sent_idx]):\n",
    "        print('sequence len error')\n",
    "        print(raw_char[sent_idx])\n",
    "        print(sequences[sent_idx])\n",
    "    for word_idx in range(len(raw[sent_idx])):\n",
    "        if len(raw_char[sent_idx][word_idx]) != len(raw[sent_idx][word_idx]):\n",
    "            print('word len error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Character vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "OL9BX1ZEK2RN",
    "outputId": "e714327e-05b6-4679-ca82-07942de92ed2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Charset dimension: 94\n",
      "Charset: abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "# NOTE: Tokenizer may take an argument char_level=True. We should try it in \n",
    "# order to get a cleaner code, but in this way we do not have a fixed length\n",
    "# for words.\n",
    "char_tokenizer = Tokenizer(lower=False, filters='')\n",
    "# Build a list with all the characters\n",
    "charset = string.ascii_letters + string.digits + string.punctuation\n",
    "print(f'Charset dimension: {len(charset)}')\n",
    "print(f'Charset: {charset}')\n",
    "char_tokenizer.fit_on_texts(list(charset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u-i2B1_xMp1y"
   },
   "outputs": [],
   "source": [
    "# Add padding to the tokenizer with the 0 integer encoding\n",
    "char_tokenizer.index_word[0] = '_PAD_'\n",
    "char_tokenizer.word_index['_PAD_'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y68BSHYmM9-B"
   },
   "source": [
    "#### Pad sentences\n",
    "Set the lengths to `max_sentence_len` (60) with padding and truncate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "vE54ewxXM3XD",
    "outputId": "4f11bf5e-3f98-460c-bbfd-e8ff7c80f168"
   },
   "outputs": [],
   "source": [
    "for sent_idx in range(len(raw_char)):\n",
    "    if len(raw_char[sent_idx]) > max_sentence_len:\n",
    "        # Truncate long sentences\n",
    "        raw_char[sent_idx] = raw_char[sent_idx][:max_sentence_len]\n",
    "    while len(raw_char[sent_idx]) < max_sentence_len:\n",
    "        # Pad sentences with '_PAD_' characters\n",
    "        pad_word = []\n",
    "        pad_word.append(char_tokenizer.index_word[0])\n",
    "        raw_char[sent_idx].append(pad_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "vE54ewxXM3XD",
    "outputId": "4f11bf5e-3f98-460c-bbfd-e8ff7c80f168"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padded sentence:\n",
      "['T', 'h', 'o', 'u', 's', 'a', 'n', 'd', 's']\n",
      "['o', 'f']\n",
      "['d', 'e', 'm', 'o', 'n', 's', 't', 'r', 'a', 't', 'o', 'r', 's']\n",
      "['h', 'a', 'v', 'e']\n",
      "['m', 'a', 'r', 'c', 'h', 'e', 'd']\n",
      "['t', 'h', 'r', 'o', 'u', 'g', 'h']\n",
      "['L', 'o', 'n', 'd', 'o', 'n']\n",
      "['t', 'o']\n",
      "['p', 'r', 'o', 't', 'e', 's', 't']\n",
      "['t', 'h', 'e']\n",
      "['w', 'a', 'r']\n",
      "['i', 'n']\n",
      "['I', 'r', 'a', 'q']\n",
      "['a', 'n', 'd']\n",
      "['d', 'e', 'm', 'a', 'n', 'd']\n",
      "['t', 'h', 'e']\n",
      "['w', 'i', 't', 'h', 'd', 'r', 'a', 'w', 'a', 'l']\n",
      "['o', 'f']\n",
      "['B', 'r', 'i', 't', 'i', 's', 'h']\n",
      "['t', 'r', 'o', 'o', 'p', 's']\n",
      "['f', 'r', 'o', 'm']\n",
      "['t', 'h', 'a', 't']\n",
      "['c', 'o', 'u', 'n', 't', 'r', 'y']\n",
      "['.']\n",
      "['T', 'h', 'o', 'u', 's', 'a', 'n', 'd', 's']\n",
      "['o', 'f']\n",
      "['d', 'e', 'm', 'o', 'n', 's', 't', 'r', 'a', 't', 'o', 'r', 's']\n",
      "['h', 'a', 'v', 'e']\n",
      "['m', 'a', 'r', 'c', 'h', 'e', 'd']\n",
      "['t', 'h', 'r', 'o', 'u', 'g', 'h']\n",
      "['L', 'o', 'n', 'd', 'o', 'n']\n",
      "['t', 'o']\n",
      "['p', 'r', 'o', 't', 'e', 's', 't']\n",
      "['t', 'h', 'e']\n",
      "['w', 'a', 'r']\n",
      "['i', 'n']\n",
      "['I', 'r', 'a', 'q']\n",
      "['a', 'n', 'd']\n",
      "['d', 'e', 'm', 'a', 'n', 'd']\n",
      "['t', 'h', 'e']\n",
      "['w', 'i', 't', 'h', 'd', 'r', 'a', 'w', 'a', 'l']\n",
      "['o', 'f']\n",
      "['B', 'r', 'i', 't', 'i', 's', 'h']\n",
      "['t', 'r', 'o', 'o', 'p', 's']\n",
      "['f', 'r', 'o', 'm']\n",
      "['t', 'h', 'a', 't']\n",
      "['c', 'o', 'u', 'n', 't', 'r', 'y']\n",
      "['.']\n",
      "['_PAD_']\n",
      "['_PAD_']\n",
      "['_PAD_']\n",
      "['_PAD_']\n",
      "['_PAD_']\n",
      "['_PAD_']\n",
      "['_PAD_']\n",
      "['_PAD_']\n",
      "['_PAD_']\n",
      "['_PAD_']\n",
      "['_PAD_']\n",
      "['_PAD_']\n"
     ]
    }
   ],
   "source": [
    "print('Padded sentence:')\n",
    "for token in raw_char[0]:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "hA6-2tB9NsuI",
    "outputId": "d5c44943-d589-4132-ae3f-83116642b1ef"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(char_tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZmFx1qRLoP5G"
   },
   "source": [
    "#### Encode characters with integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wF5ykXXaoceE"
   },
   "outputs": [],
   "source": [
    "char_seq = []\n",
    "for sentence in raw_char:\n",
    "    char_seq.append(char_tokenizer.texts_to_sequences(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 894
    },
    "colab_type": "code",
    "id": "epsNNx8VNxO3",
    "outputId": "efbfdcc9-9ad8-4ca6-acb7-33d4b2223540"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 8, 15, 21, 19, 1, 14, 4, 19] => ['T', 'h', 'o', 'u', 's', 'a', 'n', 'd', 's']\n",
      "[15, 6] => ['o', 'f']\n",
      "[4, 5, 13, 15, 14, 19, 20, 18, 1, 20, 15, 18, 19] => ['d', 'e', 'm', 'o', 'n', 's', 't', 'r', 'a', 't', 'o', 'r', 's']\n",
      "[8, 1, 22, 5] => ['h', 'a', 'v', 'e']\n",
      "[13, 1, 18, 3, 8, 5, 4] => ['m', 'a', 'r', 'c', 'h', 'e', 'd']\n",
      "[20, 8, 18, 15, 21, 7, 8] => ['t', 'h', 'r', 'o', 'u', 'g', 'h']\n",
      "[38, 15, 14, 4, 15, 14] => ['L', 'o', 'n', 'd', 'o', 'n']\n",
      "[20, 15] => ['t', 'o']\n",
      "[16, 18, 15, 20, 5, 19, 20] => ['p', 'r', 'o', 't', 'e', 's', 't']\n",
      "[20, 8, 5] => ['t', 'h', 'e']\n",
      "[23, 1, 18] => ['w', 'a', 'r']\n",
      "[9, 14] => ['i', 'n']\n",
      "[35, 18, 1, 17] => ['I', 'r', 'a', 'q']\n",
      "[1, 14, 4] => ['a', 'n', 'd']\n",
      "[4, 5, 13, 1, 14, 4] => ['d', 'e', 'm', 'a', 'n', 'd']\n",
      "[20, 8, 5] => ['t', 'h', 'e']\n",
      "[23, 9, 20, 8, 4, 18, 1, 23, 1, 12] => ['w', 'i', 't', 'h', 'd', 'r', 'a', 'w', 'a', 'l']\n",
      "[15, 6] => ['o', 'f']\n",
      "[28, 18, 9, 20, 9, 19, 8] => ['B', 'r', 'i', 't', 'i', 's', 'h']\n",
      "[20, 18, 15, 15, 16, 19] => ['t', 'r', 'o', 'o', 'p', 's']\n",
      "[6, 18, 15, 13] => ['f', 'r', 'o', 'm']\n",
      "[20, 8, 1, 20] => ['t', 'h', 'a', 't']\n",
      "[3, 15, 21, 14, 20, 18, 25] => ['c', 'o', 'u', 'n', 't', 'r', 'y']\n",
      "[76] => ['.']\n",
      "[46, 8, 15, 21, 19, 1, 14, 4, 19] => ['T', 'h', 'o', 'u', 's', 'a', 'n', 'd', 's']\n",
      "[15, 6] => ['o', 'f']\n",
      "[4, 5, 13, 15, 14, 19, 20, 18, 1, 20, 15, 18, 19] => ['d', 'e', 'm', 'o', 'n', 's', 't', 'r', 'a', 't', 'o', 'r', 's']\n",
      "[8, 1, 22, 5] => ['h', 'a', 'v', 'e']\n",
      "[13, 1, 18, 3, 8, 5, 4] => ['m', 'a', 'r', 'c', 'h', 'e', 'd']\n",
      "[20, 8, 18, 15, 21, 7, 8] => ['t', 'h', 'r', 'o', 'u', 'g', 'h']\n",
      "[38, 15, 14, 4, 15, 14] => ['L', 'o', 'n', 'd', 'o', 'n']\n",
      "[20, 15] => ['t', 'o']\n",
      "[16, 18, 15, 20, 5, 19, 20] => ['p', 'r', 'o', 't', 'e', 's', 't']\n",
      "[20, 8, 5] => ['t', 'h', 'e']\n",
      "[23, 1, 18] => ['w', 'a', 'r']\n",
      "[9, 14] => ['i', 'n']\n",
      "[35, 18, 1, 17] => ['I', 'r', 'a', 'q']\n",
      "[1, 14, 4] => ['a', 'n', 'd']\n",
      "[4, 5, 13, 1, 14, 4] => ['d', 'e', 'm', 'a', 'n', 'd']\n",
      "[20, 8, 5] => ['t', 'h', 'e']\n",
      "[23, 9, 20, 8, 4, 18, 1, 23, 1, 12] => ['w', 'i', 't', 'h', 'd', 'r', 'a', 'w', 'a', 'l']\n",
      "[15, 6] => ['o', 'f']\n",
      "[28, 18, 9, 20, 9, 19, 8] => ['B', 'r', 'i', 't', 'i', 's', 'h']\n",
      "[20, 18, 15, 15, 16, 19] => ['t', 'r', 'o', 'o', 'p', 's']\n",
      "[6, 18, 15, 13] => ['f', 'r', 'o', 'm']\n",
      "[20, 8, 1, 20] => ['t', 'h', 'a', 't']\n",
      "[3, 15, 21, 14, 20, 18, 25] => ['c', 'o', 'u', 'n', 't', 'r', 'y']\n",
      "[76] => ['.']\n",
      "[0] => ['_PAD_']\n",
      "[0] => ['_PAD_']\n",
      "[0] => ['_PAD_']\n",
      "[0] => ['_PAD_']\n",
      "[0] => ['_PAD_']\n",
      "[0] => ['_PAD_']\n",
      "[0] => ['_PAD_']\n",
      "[0] => ['_PAD_']\n",
      "[0] => ['_PAD_']\n",
      "[0] => ['_PAD_']\n",
      "[0] => ['_PAD_']\n",
      "[0] => ['_PAD_']\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(char_seq[0])):\n",
    "    w = [char_tokenizer.index_word[letter] for letter in char_seq[0][i]]\n",
    "    print(char_seq[0][i], '=>', w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UkFWAk6IOV-v"
   },
   "source": [
    "#### Pad words \n",
    "Set all the words to `maxlen` with padding and (possibly without) truncate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ONfUhZcHOVem"
   },
   "outputs": [],
   "source": [
    "def pad_words(sentence, maxlen, pad=0):\n",
    "    padded_sentence = []\n",
    "    for word in sentence:\n",
    "        new_word = word.copy()\n",
    "        if len(word) > maxlen:\n",
    "            new_word = word[:maxlen]\n",
    "        else:\n",
    "            while maxlen - len(new_word) > 1:\n",
    "                new_word.append(pad)\n",
    "                new_word.insert(0, pad)\n",
    "            if maxlen - len(new_word) == 1:\n",
    "                new_word.insert(0, pad)\n",
    "        padded_sentence.append(new_word)\n",
    "    \n",
    "    return padded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "zu6KDDBuO-Yf",
    "outputId": "96a1e8d9-75d0-4500-ed37-51668e1d530d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word length: 64\n"
     ]
    }
   ],
   "source": [
    "max_word_len = max([len(word) for word in token_tokenizer.word_index.keys()])\n",
    "print('Word length:', max_word_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PqbkIqVCvlKM"
   },
   "outputs": [],
   "source": [
    "X_char = np.array([pad_words(sentence, maxlen=max_word_len) for sentence in char_seq])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  0  0  0  0  0  0  0 46  8 15 21]\n",
      "['_PAD_', '_PAD_', '_PAD_', '_PAD_', '_PAD_', '_PAD_', '_PAD_', '_PAD_', 'T', 'h', 'o', 'u']\n",
      "==============================\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0 15]\n",
      "['_PAD_', '_PAD_', '_PAD_', '_PAD_', '_PAD_', '_PAD_', '_PAD_', '_PAD_', '_PAD_', '_PAD_', '_PAD_', 'o']\n",
      "==============================\n",
      "[ 0  0  0  0  0  0  4  5 13 15 14 19]\n",
      "['_PAD_', '_PAD_', '_PAD_', '_PAD_', '_PAD_', '_PAD_', 'd', 'e', 'm', 'o', 'n', 's']\n",
      "==============================\n",
      "[0 0 0 0 0 0 0 0 0 0 8 1]\n",
      "['_PAD_', '_PAD_', '_PAD_', '_PAD_', '_PAD_', '_PAD_', '_PAD_', '_PAD_', '_PAD_', '_PAD_', 'h', 'a']\n",
      "==============================\n",
      "[ 0  0  0  0  0  0  0  0  0 13  1 18]\n",
      "['_PAD_', '_PAD_', '_PAD_', '_PAD_', '_PAD_', '_PAD_', '_PAD_', '_PAD_', '_PAD_', 'm', 'a', 'r']\n",
      "==============================\n",
      "[ 0  0  0  0  0  0  0  0  0 20  8 18]\n",
      "['_PAD_', '_PAD_', '_PAD_', '_PAD_', '_PAD_', '_PAD_', '_PAD_', '_PAD_', '_PAD_', 't', 'h', 'r']\n",
      "==============================\n",
      "[ 0  0  0  0  0  0  0  0  0 38 15 14]\n",
      "['_PAD_', '_PAD_', '_PAD_', '_PAD_', '_PAD_', '_PAD_', '_PAD_', '_PAD_', '_PAD_', 'L', 'o', 'n']\n",
      "==============================\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0 20]\n",
      "['_PAD_', '_PAD_', '_PAD_', '_PAD_', '_PAD_', '_PAD_', '_PAD_', '_PAD_', '_PAD_', '_PAD_', '_PAD_', 't']\n",
      "==============================\n",
      "[ 0  0  0  0  0  0  0  0  0 16 18 15]\n",
      "['_PAD_', '_PAD_', '_PAD_', '_PAD_', '_PAD_', '_PAD_', '_PAD_', '_PAD_', '_PAD_', 'p', 'r', 'o']\n",
      "==============================\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0 20]\n",
      "['_PAD_', '_PAD_', '_PAD_', '_PAD_', '_PAD_', '_PAD_', '_PAD_', '_PAD_', '_PAD_', '_PAD_', '_PAD_', 't']\n",
      "==============================\n",
      "[...]\n"
     ]
    }
   ],
   "source": [
    "for word in X_char[0][:10]:\n",
    "    print(word[25:35])\n",
    "    print([char_tokenizer.index_word[char] for char in word[25:35]])\n",
    "    print('='*30)\n",
    "print('[...]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F_TnLzE3Pp-W"
   },
   "outputs": [],
   "source": [
    "# Sanity check of preprocessed data dimensions. If it does not output anything,\n",
    "# everything is fine.\n",
    "for sentence in X_char:\n",
    "    if len(sentence) != max_sentence_len:\n",
    "        print('sentence error')\n",
    "    for word in sentence:\n",
    "        if len(word) != max_word_len:\n",
    "            print(f'word error: {len(word)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V3fqZ0_opkhx"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bXVl5Praplz4"
   },
   "source": [
    "# Model implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bWXt5ji7xmRD"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, TimeDistributed, Dropout, Input, \\\n",
    "    MaxPooling1D, Flatten, concatenate, Bidirectional, LSTM, Dense\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tf2crf import CRF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "73W5p2gcJ5kb"
   },
   "source": [
    "#### Hyperparameters of the model\n",
    "You can choose between the parametrization of two proposed models:\n",
    "* Ma, Xuezhe, and Eduard Hovy. \"End-to-end sequence labeling via bi-directional lstm-cnns-crf.\" *arXiv preprint arXiv:1603.01354* (2016).\n",
    "* Chiu, Jason PC, and Eric Nichols. \"Named entity recognition with bidirectional LSTM-CNNs.\" *Transactions of the Association for Computational Linguistics 4* (2016): 357-370.\n",
    "The first works better, but it may be because second originally included the use of additional word features that we don't consider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CHIU_CONFIG = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dywNEZQjpjxo"
   },
   "outputs": [],
   "source": [
    "if USE_CHIU_CONFIG:\n",
    "    char_embedding_dim = 25\n",
    "    cnn_window_size = 3\n",
    "    cnn_filters_number = 53\n",
    "\n",
    "    word_embedding_dim = 100\n",
    "    hidden_cells = 275\n",
    "    drop=0.68\n",
    "\n",
    "    batch_size = 9\n",
    "    epochs = 80\n",
    "else:\n",
    "    char_embedding_dim = 30\n",
    "    cnn_window_size = 3\n",
    "    cnn_filters_number = 30\n",
    "\n",
    "    word_embedding_dim = 100\n",
    "    hidden_cells = 200\n",
    "    drop=0.5\n",
    "\n",
    "    batch_size = 10\n",
    "    epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "iop9KKVsBdkN",
    "outputId": "9701b9e3-8859-4412-c616-4564b842cfc3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence token length: 60\n",
      "Word character length: 64\n"
     ]
    }
   ],
   "source": [
    "print('Sentence token length:', max_sentence_len)\n",
    "print('Word character length:', max_word_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zmnpU7dNtTj-"
   },
   "source": [
    "## CNN\n",
    "We use a Convolutional Neural Network in order to extract pattern informations from the letters of the word. The CNN embedding is composed of:\n",
    "* A `keras.layers.Embedding` layer, which is a lookup table that associate a vector to each character;\n",
    "* A 1-dimensional convolution on the embedding vectors in order to capture patterns in letters;\n",
    "* A MaxPool1d that transforms a series of vectors in a unique vectors which contains informations from the characters of the word. \n",
    "\n",
    "Credits to the author of [this repo](https://github.com/kamalkraj/Named-Entity-Recognition-with-Bidirectional-LSTM-CNNs/blob/master/nn.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KMJjHLHXw3KK"
   },
   "outputs": [],
   "source": [
    "cnn_input = Input(shape=(max_sentence_len, max_word_len,), name='char_encoding')\n",
    "# We use TimeDistributed layer because we have two level of sequences:\n",
    "# * The sentence is a sequence of words;\n",
    "# * The word is a sequence of characters;\n",
    "# We want to work on the lowest sequence. the sequence of characters, so the\n",
    "# TimeDistributed layer allow us to apply this model to each word. \n",
    "cnn = TimeDistributed(Embedding(len(char_tokenizer.word_index), char_embedding_dim), name='cnn_Embedding')(cnn_input)\n",
    "cnn = Dropout(drop)(cnn)\n",
    "cnn = TimeDistributed(Conv1D(filters=cnn_filters_number, kernel_size=cnn_window_size, padding='same'), name='cnn_Convolution1d')(cnn)\n",
    "cnn = TimeDistributed(MaxPooling1D(max_word_len), name='cnn_MaxPooling1d')(cnn)\n",
    "# We finally obtain a 30-dimensional vector for each word which contains \n",
    "# char-level informations!\n",
    "cnn_out = TimeDistributed(Flatten(), name='cnn_Flatten')(cnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uI79cLjoHK4i"
   },
   "source": [
    "## Glove\n",
    "We load Glove embedding in order to embed tokens and capture word-level informations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "lc1MVNBEptQ7",
    "outputId": "3b4323da-1aac-4138-bf7c-c43fd8919863"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400001 word vectors.\n"
     ]
    }
   ],
   "source": [
    "glove_embedding_path = os.path.join('embeddings', 'glove.6B.100d.txt')\n",
    "embedding_dim = 100\n",
    "embedding_matrix = kerasutils.load_glove_embedding_matrix(glove_embedding_path, token_tokenizer.word_index, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uTGqWRvULQOZ"
   },
   "outputs": [],
   "source": [
    "word_input = Input(shape=(max_sentence_len,), name='word_encoding')\n",
    "word_embed = Embedding(len(token_tokenizer.word_index)+1, word_embedding_dim, \n",
    "                       weights=[embedding_matrix], input_length=max_sentence_len,\n",
    "                       trainable=True, mask_zero=True, \n",
    "                       name='Glove_Embedding')(word_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sJL8K7CkK9XZ"
   },
   "source": [
    "# BiLSTM + CRF\n",
    "We concatenate character- and word-level informations and pass it to a bidirectional LSTM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S-gjEgMfK8oL"
   },
   "outputs": [],
   "source": [
    "x = concatenate([word_embed, cnn_out], axis=-1)\n",
    "x = Dropout(drop)(x)\n",
    "x = Bidirectional(LSTM(hidden_cells, return_sequences=True, dropout=drop))(x)\n",
    "x = Dense(len(output_labels), activation='relu', name='Dense_Layer')(x)\n",
    "crf = CRF(len(output_labels), dtype='float32', name='CRF_Layer')\n",
    "out = crf(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Bvj86RdcyAFv"
   },
   "outputs": [],
   "source": [
    "model = Model(\n",
    "    inputs=[cnn_input, word_input],\n",
    "    outputs=out\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 605
    },
    "colab_type": "code",
    "id": "hR-_OjLb1Xk0",
    "outputId": "1d3b5c2f-3a9c-4511-ea32-385590ece220"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "char_encoding (InputLayer)      [(None, 60, 64)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "cnn_Embedding (TimeDistributed) (None, 60, 64, 30)   2850        char_encoding[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 60, 64, 30)   0           cnn_Embedding[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "cnn_Convolution1d (TimeDistribu (None, 60, 64, 30)   2730        dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "word_encoding (InputLayer)      [(None, 60)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "cnn_MaxPooling1d (TimeDistribut (None, 60, 1, 30)    0           cnn_Convolution1d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Glove_Embedding (Embedding)     (None, 60, 100)      2742100     word_encoding[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "cnn_Flatten (TimeDistributed)   (None, 60, 30)       0           cnn_MaxPooling1d[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 60, 130)      0           Glove_Embedding[0][0]            \n",
      "                                                                 cnn_Flatten[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 60, 130)      0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional (Bidirectional)   (None, 60, 400)      529600      dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Dense_Layer (Dense)             (None, 60, 18)       7218        bidirectional[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "CRF_Layer (CRF)                 (None, 60)           324         Dense_Layer[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 3,284,822\n",
      "Trainable params: 3,284,822\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(\n",
    "    loss=crf.loss, \n",
    "    optimizer='adam',\n",
    "    metrics=[crf.accuracy]\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BliGFagr3tNE"
   },
   "outputs": [],
   "source": [
    "# Early stopping\n",
    "early_stopping_callback = EarlyStopping(monitor=\"val_loss\",\n",
    "                                        patience=3, min_delta=0.001, verbose=1, \n",
    "                                        restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Train-test split\n",
    "X_sent_train, X_sent_test, Y_train, Y_test = train_test_split(X_sent, Y, test_size=0.2, random_state=42)\n",
    "X_char_train, X_char_test, _, _ = train_test_split(X_char, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 689
    },
    "colab_type": "code",
    "id": "4m1DOCgVHgom",
    "outputId": "e9d09231-c5f1-4cf7-fbca-6b62a130681a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "2252/2252 [==============================] - 266s 118ms/step - loss: 3.7094 - accuracy: 0.9276 - val_loss: 27.7781 - val_accuracy: 0.9619\n",
      "Epoch 2/20\n",
      "2252/2252 [==============================] - 263s 117ms/step - loss: 1.7046 - accuracy: 0.9572 - val_loss: 23.9774 - val_accuracy: 0.9649\n",
      "Epoch 3/20\n",
      "2252/2252 [==============================] - 270s 120ms/step - loss: 1.3612 - accuracy: 0.9623 - val_loss: 20.4389 - val_accuracy: 0.9672\n",
      "Epoch 4/20\n",
      "2252/2252 [==============================] - 273s 121ms/step - loss: 1.1791 - accuracy: 0.9655 - val_loss: 17.8885 - val_accuracy: 0.9683\n",
      "Epoch 5/20\n",
      "2252/2252 [==============================] - 267s 119ms/step - loss: 1.0751 - accuracy: 0.9675 - val_loss: 16.4535 - val_accuracy: 0.9675\n",
      "Epoch 6/20\n",
      "2252/2252 [==============================] - 267s 119ms/step - loss: 1.0012 - accuracy: 0.9688 - val_loss: 15.7356 - val_accuracy: 0.9676\n",
      "Epoch 7/20\n",
      "2252/2252 [==============================] - 278s 124ms/step - loss: 0.9497 - accuracy: 0.9706 - val_loss: 15.8333 - val_accuracy: 0.9678\n",
      "Epoch 8/20\n",
      "2252/2252 [==============================] - 273s 121ms/step - loss: 0.8977 - accuracy: 0.9716 - val_loss: 16.3475 - val_accuracy: 0.9690\n",
      "Epoch 9/20\n",
      "2252/2252 [==============================] - ETA: 0s - loss: 0.8627 - accuracy: 0.9726Restoring model weights from the end of the best epoch.\n",
      "2252/2252 [==============================] - 275s 122ms/step - loss: 0.8627 - accuracy: 0.9726 - val_loss: 17.2223 - val_accuracy: 0.9691\n",
      "Epoch 00009: early stopping\n"
     ]
    }
   ],
   "source": [
    "history = model.fit([X_char_train, X_sent_train],\n",
    "    Y_train, \n",
    "    batch_size=batch_size, \n",
    "    epochs=epochs,\n",
    "    verbose=1,\n",
    "    callbacks=[early_stopping_callback],\n",
    "    validation_split=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Gg3qkwMFhaEB"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yYvfaGaVhb5-"
   },
   "source": [
    "## Evaluation\n",
    "We evaluate three aspects of the model:\n",
    "* **Memory consumption** using the `kerasutils.print_model_memory_usage()` function (found [here](https://stackoverflow.com/questions/43137288/how-to-determine-needed-memory-of-keras-model));\n",
    "* **Latency in prediction** using the function `time.process_time()`;\n",
    "* **F1-score** _on entities_ on the test set using `seqeval`;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "qH5-b5wRXRpx",
    "outputId": "7686d741-d915-423f-88f7-5a3963bef76e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 27.783 MB\n"
     ]
    }
   ],
   "source": [
    "kerasutils.print_model_memory_usage(batch_size, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "XjCR4W95Xa2l",
    "outputId": "0343233e-f9b7-47f4-e679-d41c209d0cd3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model latency in predictions: 0.00779 s\n"
     ]
    }
   ],
   "source": [
    "print(f'Model latency in predictions: {modelutils.compute_prediction_latency([X_char_test, X_sent_test], model, n_instances=len(X_sent_test)):.3} s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 924
    },
    "colab_type": "code",
    "id": "GjTgiaPWX96F",
    "outputId": "98ce18a5-5d8c-495e-d4f0-e4ecc4eb394d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set\n",
      "           precision    recall  f1-score   support\n",
      "\n",
      "      org      0.793     0.723     0.757     15970\n",
      "      gpe      0.968     0.928     0.948     12914\n",
      "      tim      0.832     0.888     0.859     15898\n",
      "      per      0.807     0.851     0.828     13596\n",
      "      geo      0.871     0.912     0.891     29297\n",
      "      nat      0.670     0.335     0.447       176\n",
      "      art      0.000     0.000     0.000       355\n",
      "      eve      0.482     0.303     0.372       267\n",
      "\n",
      "micro avg      0.853     0.860     0.856     88473\n",
      "macro avg      0.849     0.860     0.854     88473\n",
      "\n",
      "\n",
      "\n",
      "Test Set\n",
      "           precision    recall  f1-score   support\n",
      "\n",
      "      gpe      0.960     0.930     0.945      3260\n",
      "      tim      0.812     0.860     0.835      3987\n",
      "      org      0.752     0.682     0.715      3950\n",
      "      geo      0.857     0.896     0.876      7580\n",
      "      nat      0.500     0.224     0.310        49\n",
      "      per      0.771     0.817     0.794      3265\n",
      "      eve      0.452     0.206     0.283        68\n",
      "      art      0.000     0.000     0.000        71\n",
      "\n",
      "micro avg      0.832     0.838     0.835     22230\n",
      "macro avg      0.828     0.838     0.832     22230\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from seqeval.metrics import classification_report\n",
    "\n",
    "\n",
    "datasets = [('Training Set', X_char_train, X_sent_train, Y_train), \n",
    "            ('Test Set', X_char_test, X_sent_test, Y_test)]\n",
    "\n",
    "for title, X_char, X_sent, Y in datasets:\n",
    "    Y_pred = model.predict({'char_encoding': X_char, 'word_encoding': X_sent}, batch_size=batch_size)\n",
    "    Y, Y_pred = kerasutils.remove_seq_padding(X_sent, Y, Y_pred)\n",
    "    Y, Y_pred = modelutils.from_encode_to_literal_labels(Y, Y_pred, idx2tag)\n",
    "    print(title)\n",
    "    print(classification_report(Y, Y_pred, digits=3))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Glove+CNN+BiLSTM+CRF_CoNLL.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
